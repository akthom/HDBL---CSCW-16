\section{Cases}

\subsection{Demographic summary}

Every single one of our participants was in the middle of, or preparing for, a database migration of some sort; this was somewhat surprising, given that (with one exception) we did not actively seek out participants in the middle of migrations . Additionally, all of our participants learned database management and design on-the-job (sometimes with the help of a text book purchased with their own funds) and have worked with databases exclusively within the context of NHMs or GIS; several have taken short workshops in GIS or database design, and though two took “a few” computer science classes in college, these did not focus on database design or digital preservation. Our participants’ years of experience with databases ranged from 2 to 30; database systems included Access, FileMaker Pro, MySQL Workbench, Arctos, Specify, and Excel.

Over 36 databases were described by our participants (see Appendix A for a full description of the type and format of each database throughout our three phases of observation); perhaps not surprisingly, collections management databases were the most common type of database being managed and migrated by our participants. All but one of the collections management databases had recently, or was about to be migrated to either Specify or Arctos, both of which are free, open source relational database systems designed specifically for NHMs. Several participants additionally maintained transactional (one participant called them “work tasks”) databases separate from their collections databases, used largely to manage loan documents and print labels – even though the collections management databases ostensibly have the ability to print labels or loan documents. It seems that the legacy reporting or label printing functions that users maintained in Access simply couldn’t be migrated easily to systems like Specify or Arctos. As a work around, CMs export data from their primary database, and importing it into Access or Excel to create labels and forms. The transactional databases don’t seem to be viewed as needing migration. Finally, five participants managed research and literature databases. These often overlapped or interlinked with collections databases, but were largely being maintained in “custom” databases (e.g. in MySQL or Access, rather than Specify or Arctos). Two of these (MioMap and the UCD database) are actively being integrated with larger community-driven projects; two are being maintained as they are (the Decapod literature database and the NMITA database); and one (MapStedi) may be archived as it is, pending the hiring and decision of a new curator.

\subsection{How each database learned}

\subsubsection{Vertebrate Zoology, CU Boulder}

\begin{tabular}{ c c c c c c c c c }
    "Database Name" & "Format at CM's arrival at museum" & "Present Format" & "Anticipated Future Format & Type" \\ 
    A set of
  databases divided by type of vertebrate (e.g. birds, mammals, fish, reptiles)
  and by collections' function (e.g. Research, University Teaching, Exhibit,
  K-12 Teaching) & A set of
  databases divided by type of vertebrate (e.g. birds, mammals, fish, reptiles)
  and by collections' function (e.g. Research, University Teaching, Exhibit,
  K-12 Teaching) & 16 FMP
  files & 16 FMP
  files & Excel,
  cleaning with Open Refine & Excel,
  cleaning with Open Refine & Arctos & Arctos & Collections Management database \\ 
    A set of
  databases divided by type of vertebrate (e.g. birds, mammals, fish, reptiles)
  and by collections' function (e.g. Research, University Teaching, Exhibit,
  K-12 Teaching) \\ 
    16 FMP
  files \\ 
    Excel,
  cleaning with Open Refine \\ 
    Arctos \\ 
\end{tabular}



The collections are split into 4 categories (Birds, Mammals, Reptiles & Fish), which are in turn split into 4 databases (Research, Exhibits, K-12 Education, University Education). Each database is a separate FileMaker Pro File; 16 databases in total.  At the time of our first interview, she was migrating and merging these databases into Arctos, but needs to clean the data considerably; a year later she had successfully finished this project, and moved to physically migrating her specimen collections from one room to another.

How she learned the databases, and how the databases learned: The databases were largely undocumented when Braker began work at CU, and as a result she had to learn each database’s structure through qualitative exploration, review of records one-by-one, and by conferring with more other staff.  She found many duplicate records between the databases (the same specimen would be cataloged in the “research” and “education” databases, with different information in each), and that this was “an artifact of trying to get things... digitized” without understanding the long-term implications of early database design choices.   This siloing, though, served an administrative function: it was a simple way of allowing different departments to have control over different aspects of the database.  Braker faced some significant challenges in migrating the databases to Arctos, largely due to differences in data structure rooted in underlying physical curatorial practices.  Fish and reptiles are often stored in jars of alcohol and cataloged as “lots” of multiple specimens, whereas birds and mammals are more likely to be cataloged individually, or even dissected into constituent parts.  Thus, the migration to Arctos forced Braker to migrate several different schemas to one unified schema.

Invertebrate Zoology, CU Boulder. CM Heather Robeson began working with databases for data entry 6 years ago, but did not begin managing and maintaining them until 3 years ago, when she began her job as CM.  Her collections catalogs are primarily split into three FileMaker Pro databases, which mirror her collections’ physical organization -- Mollusk, non-mollusk invertebrate, and type specimens; she has additionally inherited several peripheral databases, such as a “taxonomy” database containing the master list of taxonomic names, and a research database created through an NSF-funded project (Mapstedi; REF), of which the former Invertebrate Zoology curator was the PI.  For the last year, she, too, has been working to clean her data sufficiently to migrate to Arctos (independently from Braker, though she did consult with Braker before making the choice to migrate to Arctos as opposed to another platform). 

 How she learned the databases, and how the databases learned: Robeson learned the database’s structure while simultaneously learning the basics of relational database management: on-the-job, and with the help of a textbook. One of the primary challenges she faced: deciphering and normalizing the links between the collections databases, and the peripheral databases.  The primary key on the taxonomy table, for instance, needed to be moved from the “genus” to an independent identifier (to allow for identifications that could be made at the genus level).  That said: though she’s learned much about relational database design and use, she has found that Excel has been much easier to use for data cleaning and curation tasks, and has been maintaining the databases as spreadsheets for the last two years.  Migration is presently on hold until a new curator can be hired and consulted regarding migration decisions (e.g. what to do with the MapStedi database).  As of now, she is the primary (if not sole) curator and user of this database.

Invertebrate Paleontology, CU Boulder: CM Dr Talia Karim began working with databases over 6 years ago, while working as Invertebrate Paleontology Collections Manager at the University of Kansas Biodiversity Institute (home of Specify, a popular, free NHM collection management platform). She manages only one database presently: the Invertebrate Paleontology Collections database.

How she learned the database, and how the database learned: Karim’s prior work at KU prepared her for work with the CU database: they’re both in Specify.  That said, while they both must use the same Specify schema, they have been used in different ways.  One of Karim’s first tasks at CU was migrating the database from Specify 5 to 6; as is fairly common in Specify, despite being developed specifically for NHMs, many CMs co-opt fields for their own uses. When she migrated her database from 5 to 6, she had to compare field names in data entry forms “by hand” to the field names in the underlying schema to ensure that they will “map” properly to the new system: 
"whoever had set up the 5 databases had co-opted fields, so the field that was hard coded into the database of, like, "latitude" may not be where latitude was actually stored.  So when [the Specify team] went to do the mapping, they mapped latitude to latitude -- but it turned out latitude in 5 was in "text field 2" or something." 
Karim is now anticipating a further migration to Specify 7 – and with it, another change in schema. In Specify 5, stratigraphic and geologic time data were linked to locality records (the place from which a specimen was collected).  In 6, the Specify developers changed this to link stratigraphic information and geologic time to each specimen (this would resolve some data modeling issues for core data).  This change has been met with considerable opposition by many paleontology collections; it means that locality data must be entered with each specimen, hugely increasing the time spent on data entry. Karim is hoping Specify developers will change the schema soon, and is waiting until then to migrate to the new system .  

Karim is the primary manager and user of this database, but does rely on students for much of the data entry (they are adding images to many of the specimen records as part of a digitization project).  Students enter data into the database directly, but she does check over their work regularly. Additionally the CU IT department must manage all software updates.

Vertebrate Paleontology, U of Oregon: Curator Dr Edward Davis began working with databases during his doctoral work at UC Berkeley; his advisor, Tony Barnosky, passed the MioMap project down to him (http://www.ucmp.berkeley.edu/miomap/).  He continues to manage the MioMap database today, as well as the UO Vertebrate Paleontology Collection database.

How he learned the databases, and how the databases learned: Davis learned the MioMap database with the help of his advisor, and learned database design with the help of a textbook; he has managed the database off and on for n years, teaching himself relational design, SQL and PHP in the process.  He is now working to integrate MioMap into Neotoma, a larger, also relational, community-developed paleoecology research database as part of an NSF project aiming to create a unified API for the major paleontology resources on the web.  One challenge here: MioMap has 6 tables, whereas Neotoma has 25-30. Davis, “expect[s] there to be problems where there are elements that are going to be single lines in new tables in Neotoma that are right now multiple entries in the tables that [he has] in MioMap. And those entries aren't uniform in their machine readability."  Thus he anticipates a huge need for data curation and cleaning.

Davis’ collections database is somewhat simpler than MioMap – when he arrived at UO, it was in Access, and he has since migrated it to Specify.  That said, the migration process was thorny.  The original Access database was only relational in spirit.  "Loosely based on the UCMP's database structure," it contained two tables – one for specimens, and one for localities. However, the database’s original creator never actually saw the UCMP database:
“Ostensibly these two tables that would be linked through the locality number... there wasn't actually a link between the two tables in Access. So whoever had built it had... never actually gone through and done the job of connecting them."
Much of the locality data was repeated in the Specimen table, and over time, the locality data in each table began to drift apart, so to speak: records with the same locality number contained different locations.  During the migration to Specify, Davis and his students spent considerable time correcting this data (by cross-referencing with the paper card catalog), while also cleaning it – by hand – for issues like excessive capitalization and trailing spaces at the end of records.  

Paleobotany & micropaleontology, “Prairie State University ”: Collections Manager “Anna” has worked with databases for over 20 years, and with the databases at the University for almost 10 years.  She manages four different systems: a paleobotany & micropaleontology collections database with a “complicated history”, currently being stored in Excel while she awaits migration to MySQL; a smaller mineral collections database (currently being digitized into Excel); several EndNote libraries; an a web accessible clone of the paleobotany collections database (in Access w/ a SQL backend).

How she learned the databases, and how the databases learned:  The bulk of Anna’s time is spent working with the paleobotany & micropaleontology database: it was first transcribed from a card catalog into FileMaker Pro, and then migrated to a highly complex Access database before she arrived at the museum.  Because her curator was never comfortable with Access, Anna attempted to migrate the database to Specify. However, they quickly encountered problems mapping the Access schema to Specify’s – largely because of unique collection of “coal balls” -- "calcium carbonate concretions within coal seams" that preserve plant fossils.  As Anna describes:
"they cut them in half, etch them with hydrochloric acid, put a little bit of acetone down, put an acetate sheet on it, and peel the acetate sheet off, and then you have this thin section through the plant fossils that are in the coal balls.  You can do this multiple times for the surface, so part of the problem is you have this object that can be cut multiple times, because it doesn't just have to be in half... you can slice it like bread... and you can peel each one of the surfaces...  So how do you associate multiple IDs to the same general entity object, and how do you allow this reconstruction of all the relationships, to be able to put that coal ball back together, so that you can be able to know that this all came from the one thing?  That's really what the problem is, and why we have such complex data."
Their data model has been developed specifically to express the complicated relationships between each of these thin sections, as well as some other fairly complex stratigraphic data, and despite extensive work with the Specify team, she was not able to crosswalk the schema.  She has since decided to build a custom MySQL database , but is waiting for her university’s IT department to set up the server.  In the interim, she has stored each table of the database as a separate sheet within an excel file: managing and updating relations and primary keys by hand, and slowly cleaning the data in preparation for eventual ingest into MySQL. She is the primary user of the database, but will sometimes create copies of specific tables for students to work with (she notes that she must manually lock columns containing primary keys and other important ID’s, and checks their work by hand before re-integrating it with the master file).

Paleontology (both invertebrate and vertebrate) - U of Iowa:  CM Tiffany Adrain has 26 years of experience with database design and management in NHMs, and has been working with her current charges for 16 years.  Adrain manages a collections database (in Specify 5); several “work tasks” Access databases used for processing loans and printing labels; and a web-accessible Oracle research database 

While Tiffany is the only full time user/manager of this DB, she has a large number of volunteers and students working with her and on other projects.  When asked if she's the only one entering data directly into the system, she said, 
"No, but I should be!  [laughs]  It's difficult...  I always feel like I'm the bottleneck here.  If I have students doing spreadsheets, putting information into spreadsheets and giving [them] to me to put into the database, it would stop with the spreadsheets...."  

