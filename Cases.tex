\section{Cases}

\subsection{Demographic summary}

Over 36 databases were described by our participants (see Appendix A for a full description); perhaps not surprisingly, collections management databases were the most common type of database being managed by our participants. Every single one of these databases was being migrated, or prepared for migration; this was somewhat surprising, given that (with one exception\footnote{our anonymous participant at the "Prairie Museum"}) we did not actively seek out participants in the middle of migrations. Further, all but one of the collections management databases are being migrated to either Specify or Arctos, both of which are free relational database systems designed specifically for NHMs. 

Five participants additionally managed research and literature databases. These often overlapped or interlinked with collections databases, but were maintained as “custom” databases (e.g. in MySQL or Access). Two of these (MioMap and the UCD database) are actively being integrated with larger community-driven projects; two (the Decapod literature database and the NMITA database) are being maintained as they are; and one (MapStedi) may be archived as it is.  Finally, N of our participants managed transactional databases separate from their collections databases; these were typically used as bridges between the collections databases and Excel or Word for various "work tasks" (in one participant's phrasing) such as processing loan documents or printing labels. These databases were not viewed as needing migration; as such they will not be discussed in detail below.  However we will discuss their role in the information ecosystem (i hate this phrase) later.

All but one of our participants had to reverse engineer their collections databases to some degree; these databases were typically undocumented and in varying states of denormalization when they "inherited" them, so to speak, and extensive review of records one-by-one, coupled with conversations with senior staff, was necessary to understand the databases' structure. Further, all of our participants learned database management and design on-the-job; several have taken short workshops in GIS or database design, and though two took “a few” computer science classes in college, these did not focus on database design or digital preservation. Our participants’ years of experience with databases ranged from 2 to 30; database systems included Access, FileMaker Pro, MySQL Workbench, Arctos, Specify, and Excel. Below, we discuss the collections management and research databases in greater detail.

\subsubsection{Vertebrate Zoology, CU Boulder}

\begin{tabular}{ c c c c c c } 
    Database Name & Prior Formats & Format @ Interview 1 & Format @ Interview 2 & Anticipated Future Format & Type \\ 
    A set of databases divided by type of vertebrate (e.g. birds, mammals, fish, reptiles) and by collections' function (e.g. Research, University Teaching, Exhibit, K-12 Teaching) & 16 FMP files & Excel, cleaning with Open Refine & Merged into unified Arctos DB & Arctos & Collections Management \\ 
\end{tabular}

The Vertebrate Zoology collection is split into 4 subcollections (Birds, Mammals, Reptiles & Fish), which have in turn been digitized into 4 databases each (Research, Exhibits, K-12 Education, University Education). Each database is a separate FileMaker Pro File; 16 databases in total.  At the time of our first interview, CM Emily Braker was migrating and merging these databases into Arctos, but needed to clean the data considerably; at our second interview, a year later, she had successfully finished this project, and had transitioned to \textit{physically} migrating the specimen collections from one \textit{room} to another.

Braker describes these databases as mirrors of previous curatorial workflows: she says the separate databases are “an artifact of trying to get things... digitized” without understanding the long-term implications of early design choices. The silos were intended to make separate administration of each database easier (for instance, allowing exhibit staff control over their own databases); however, with duplicate databases came duplicate records -- particularly between the Reserach and Teaching databases. In order to migrate the collections databases to Arctos, Braker first exported the databases into "flat files" in Excel; from there she was able to iterate between Excel and Open Refine to cluster and clean her data, re-control vocabularies, and begin atomizing fields into smaller subsets so that they would better "match" Arctos' schema (e.g. splitting a locality description into separate county, state and country fields). Before the migration, Braker largely worked on the database herself; now that the system has been moved to Arctos, she feels more comfortable allowing students to do data entry work.

This database may be viewed as learning by mirroring existing work structures, and as a result, being created in an unnormalized state; with the migration to Arctos, the database has become far more normalized than it was in the past. While the unnormalized databases initially supported cooperative work, eventually the data became too "dirty" for anyone other than the primary collections manager to work with; with migration to a unified, stable system and considerable curation work on Braker's part, the database can be used collaboratively once again.

\subsubsection{Invertebrate Zoology, CU Boulder.}

The Invertebate Zoology collections and databases are both split into Mollusks, Non-Mollusk Invertebrates, and Type Specimens. However, CM Heather Robeson has additionally inherited several "peripheral" databases, such as a “taxonomy” database containing the master list of taxonomic names, and a research database created through an NSF-funded project led by a former curator. Robeson is not certain who created the initial collections databases in FMP; she does know that they were intended to be linked to the taxonomy table, the relations and relationships are not well-formed. The primary key on the taxonomy table, for instance, needed to be moved from the “genus” to an independent identifier (to allow for identifications that could be made at the genus level). While she was able to make some repairs to the FMP files to improve their use, she has nevertheless decided to export them to Excel as flat files for easier cleaning.  She anticipates a migration to Arctos, but this is presently on hold until a new curator can be hired and consulted regarding migration decisions (e.g. what to do with the MapStedi database).  As of now, she is the primary (if not sole) user of this database.

This database may be seen as learning to denormalize: the legacy structures were too idiosyncratic and poorly documented to be recovered, and as a result, the database has found stability as a flat file. We may additionally view this database's 'collapse' as creating a \textit{need} for collaboration; Robeson now needs to consult with museum (and system) administrators to determine how to rebuild her database.

\subsubsection{Invertebrate Paleontology, CU Boulder:}

Upon CM Karim's arrival at CU, the Invert Paleo collections database was already reasonably well curated in Specify 5.  However, when Specify released new version 6, they also changed their schema. During the migration from 5 to 6, she discovered that a great number of fields had been "co-opted":
\begin{quote}
"whoever had set up the 5 databases had co-opted fields, so the field that was hard coded into the database of, like, "latitude" may not be where latitude was actually stored.  So when [the Specify team] went to do the mapping, they mapped latitude to latitude -- but it turned out latitude in 5 was in "text field 2" or something." 
\end{quote}
As a result, she had to compare field names in data entry forms “by hand” to the field names in the underlying schema to ensure that they would “map” properly to the new system. Karim is now anticipating a further migration to Specify 7 – and with it, another change in schema. [In Specify 5, stratigraphic and geologic time data were linked to locality records (the place from which a specimen was collected). In 6, the Specify developers changed this to link stratigraphic information and geologic time to each specimen (this would resolve some data modeling issues for core data). This change has been met with considerable opposition by many paleontology collections; it means that locality data must be entered with each specimen, hugely increasing the time spent on data entry. Karim is hoping Specify developers will change the schema soon, and is waiting until then to migrate to Specify 7. - potentially delete for space and leave for discussion] Despite some of these setbacks, Karim has been able to rely on students fairly consistently throughout the database's lifespan for much of the data entry. She does, however, need to review check over their work regularly.

In this case, one can view the database as learning through iterative schema control. The initial Specify 5 installaiton provided users with a sufficiently stable platform to create a relational-enough system that worked for their idiosyncratic lab practices; the migration to 6 forced Karim to re-control the schema.  The database's consistant usability -- and current relational health -- by a range of users could be attributed to the fact that its never been allowed to be fully de-normalized.

\subsubsection{Vertebrate Paleontology, U of Oregon:}

When Davis first took over the collections database, it was in Access; and he has since migrated it to Specify 6. "Loosely based on the UCMP's database structure," it contained two tables – one for specimens, and one for localities. However, the database’s original creator never actually saw the UCMP database:
\begin{quote}
“Ostensibly these two tables that would be linked through the locality number... there wasn't actually a link between the two tables in Access. So whoever had built it had... never actually gone through and done the job of connecting them."
\end{quote}
Much of the locality data was repeated in the specimen table, and over time, the locality data in each table began to diverge: records with the same locality ID contained different locality descriptions. During the migration to Specify, Davis and his students spent considerable time correcting and cleaning this data – by hand – for issues like excessive capitalization and trailing spaces at the end of records.  This database may be viewed as learning through mimicry; concept creep; and then recontrol. While it has been collaboratively used and contributed to throughout its lifecycle, the initial collaborative data entry created the anomalies that Davis later had to fix.

Davis also manages a publicly available research database called MioMap, which he is working to integrate into Neotoma, a larger, also relational, community-developed paleoecology research database. A key challenge here: MioMap has 6 tables, whereas Neotoma has 25-30. Davis, 
\begin{quote}
“expect[s] there to be problems where there are elements that are going to be single lines in new tables in Neotoma that are right now multiple entries in the tables that [he has] in MioMap. And those entries aren't uniform in their machine readability."  
\end{quote}
Over its history, MioMap has learned in stops and starts: development is driven by funding cycles. However it can also be seen as learning through convergence: over time, it and a similar database (FaunMap) managed by a colleague have grown to parallel each other's relational structure, and now both are being integrated into the larger project. [this paragraph maybe needs deleting or discussion elsewhere]

\subsubsection{Paleobotany & micropaleontology, “Prairie State University ”:}
 Collections Manager “Anna” has worked with databases for over 20 years, and with the databases at the University for almost 10 years.  She manages four different systems: a paleobotany & micropaleontology collections database with a “complicated history”, currently being stored in Excel while she awaits migration to MySQL; a smaller mineral collections database (currently being digitized into Excel); several EndNote libraries; an a web accessible clone of the paleobotany collections database (in Access w/ a SQL backend).

How she learned the databases, and how the databases learned:  The bulk of Anna’s time is spent working with the paleobotany & micropaleontology database: it was first transcribed from a card catalog into FileMaker Pro, and then migrated to a highly complex Access database before she arrived at the museum.  Because her curator was never comfortable with Access, Anna attempted to migrate the database to Specify. However, they quickly encountered problems mapping the Access schema to Specify’s – largely because of unique collection of “coal balls” -- "calcium carbonate concretions within coal seams" that preserve plant fossils.  As Anna describes:
"they cut them in half, etch them with hydrochloric acid, put a little bit of acetone down, put an acetate sheet on it, and peel the acetate sheet off, and then you have this thin section through the plant fossils that are in the coal balls.  You can do this multiple times for the surface, so part of the problem is you have this object that can be cut multiple times, because it doesn't just have to be in half... you can slice it like bread... and you can peel each one of the surfaces...  So how do you associate multiple IDs to the same general entity object, and how do you allow this reconstruction of all the relationships, to be able to put that coal ball back together, so that you can be able to know that this all came from the one thing?  That's really what the problem is, and why we have such complex data."
Their data model has been developed specifically to express the complicated relationships between each of these thin sections, as well as some other fairly complex stratigraphic data, and despite extensive work with the Specify team, she was not able to crosswalk the schema.  She has since decided to build a custom MySQL database , but is waiting for her university’s IT department to set up the server.  In the interim, she has stored each table of the database as a separate sheet within an excel file: managing and updating relations and primary keys by hand, and slowly cleaning the data in preparation for eventual ingest into MySQL. She is the primary user of the database, but will sometimes create copies of specific tables for students to work with (she notes that she must manually lock columns containing primary keys and other important ID’s, and checks their work by hand before re-integrating it with the master file).

\subsubsection{Paleontology (both invertebrate and vertebrate) - U of Iowa:}
CM Tiffany Adrain has 26 years of experience with database design and management in NHMs, and has been working with her current charges for 16 years.  Adrain manages a collections database (in Specify 5); several “work tasks” Access databases used for processing loans and printing labels; and a web-accessible Oracle research database 

While Tiffany is the only full time user/manager of this DB, she has a large number of volunteers and students working with her and on other projects.  When asked if she's the only one entering data directly into the system, she said, 
"No, but I should be!  [laughs]  It's difficult...  I always feel like I'm the bottleneck here.  If I have students doing spreadsheets, putting information into spreadsheets and giving [them] to me to put into the database, it would stop with the spreadsheets...."  

\subsubsection{Invertebrate Zoology - LACM}
Further complicating schema migration: the Specify schema in particular is a “moving target” as one participant put it – because Specify changes its schema from version to version.  One CM (Pentcheff, from LACM) has been working on “fielding” a Specify database for almost two years now – identifying what fields he would want in forms, and what fields he could crosswalk his existing database to in the Specify schema – but his migration has been delayed both by other work obligations, and by the desire to match each new Specify release:
“I have been so interrupted for the past several years, that we have multiple times gotten to the 95% complete phase for introduction and migration into it, and then something comes up, we get distracted, and then 6 months later we try it again. … each time I've spent time on it, it takes 4 or 5 days to come back up to speed, then I get one day of work on it, and then my week is over, and there are people clamoring at the door and I've got to go put out some other fires.”
Though his Specify 6 database is largely mapped out, the release of Specify 7 has him considering whether to port that schema to the newer version. 


