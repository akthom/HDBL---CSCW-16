\subsection{Results}
\subsubsection{Laying the foundation: origins of databases}

Over 36 databases were described by our participants (median per department = 3; see Appendix A for a fuller description of each database). Not surprisingly, collections management databases were most common; every single one of these collections databases was being migrated, or prepared for migration \footnote{with the exception of the Prairie Museum micropaleontology collection – we did not actively seek out departments undergoing collections database migration.}. Five participants additionally managed research and literature databases. These often overlapped or interlinked with collections databases, but were maintained as “custom” databases (e.g. in MySQL or Access). N of our participants managed transactional databases separate from their collections databases; these were typically used as bridges between the collections databases and Excel or Word for various “work tasks” (in one participant’s phrasing) such as processing loan documents or printing labels.  In some cases (notably the U of Iowa’s Paleontology collection, CU Boulder’s Invertebrate Zoology collection, and the LACM marine biology database) electronic databases have essentially replaced paper catalogs; in others (CU Boulder Invertebrate Paleontology, UT Austin) they are maintained concurrently with paper catalogs, which are viewed as being stable and preservable from a curatorial standpoint.

Most of our participants were unsure when the collections databases had originally been digitized, and all but one of our participants (LACM) had to reverse engineer their collections databases to some degree; these databases were typically undocumented and in varying states of denormalization when they “inherited” them,, and extensive review of records one-by-one, coupled with conversations with senior staff, was necessary to understand the databases’ structure.  While each database’s legacy structure was somewhat unique to its institution, we found evidence of what Brand called “vernacular” learning (REF): styles – some of them, regional – of database design. [However, -- one more sentence explaining how these styles lose something in translation from one location to the next].  For instance, the University of California’s Museum of Paleontology (UCMP) at Berkeley has long been a leader in not just but NHM standards development (as Star & Griesemer describe), but also a leader in NHM computing.  The UCMP website was one of the first 100 websites online, and they were one of the first institutions to begin digitizing their collections catalogs. When participant UOVP -- a UCMP alum -- f irst took over as custodian of the UO collections database, he discovered that the UOVP database had been "loosely based on the UCMP's database structure." It contained two tables -- one for specimens, and one for localities. However, the database’s original creator never actually saw the UCMP database:
\begin{quote}
“Ostensibly these two tables that would be linked through the locality number... there wasn't actually a link between the two tables in Access. So whoever had built it had... never actually gone through and done the job of connecting them."
\end{quote}
The UT Austin collections database – also originally built by a UCMP alum – similarly features this two table, specimen/locality structure. Thus, just as Brand finds that buildings mimic one another’s facades over time, NHM databases, too, learn their structure from their peers. However, the translation isn’t always perfect: just as [find a good example of something architectural from Brand], the original creator missed something crucial in translating the UCMP schema to UOVP.  In short, changes were made to the “skin” of the database, but the corresponding and necessary changes to the structure were not. [these metaphors will be explained in the Brand section of the background] 

\subsubsection{Up to Code: denormalization for data cleaning}

Reverse engineering database structure is only the first part of the migration process: next, CMs must bring their data “up to code”, so to speak, by re-controlling vocabularies, and re-normalizing their tables. For several participants, this meant taking their data out of the database, and instead, exporting to flat-file programs like Excel and Open Refine. Some of this work simply can’t be done with existing database tools: SQL statements can’t parse irregular locality strings into atomized fields for county, state and city, or tell you whether a taxonomic name is misspelled or simply arcane.  Tools like Excel and Open Refine, on the other hand, allow users to drag and drop text; strip terminal spaces from strings; cluster and correct text; and further, as one participant noted, can be “just visually easier” to use. 

Further, Excel is sometimes used as a neutral staging area between migrations. The Invert Zoology CM at CU Boulder had such problems renormalizing her FileMaker database that she has instead exported the entire database into Excel for cleaning, and will be keeping it there until she’s able to finish cleaning the data for migration to Arctos.  Another CM, at the Prairie Museum, is similarly using Excel as holding area – but not as a flat file.  Rather, she is storing each table of her legacy database as a separate sheet within an Excel file, managing and updating relationships and primary keys by hand, until her museum’s IT department is able to set up a MySQL server for her use.

[ a few more sentences here]

\subsubsection{Retrofitting: the emergence of, and migration to, Specify and Arctos}

All but one of the collections databases described by our participants had been, or were in the process of being migrated to Arctos or Specify, or being migrated between different versions of Specify. Both of these databases are designed specifically for use with NHM collections; both are free; both “ship” with a pre-defined data schema which users cannot alter; both seem to offer fairly extensive technical support and help with migration (time permitting, Specify will even clean your data for you). However, there are some important distinctions between the two programs: Arctos publishes its users’ collections online as a large, public, aggregated database; thus, stricter adherence to the Arctos data model is necessary. Specify, on the other hand, does not publish or aggregate data for its users, and thus (for better or for worse, as we discuss further below) allows users some greater flexibility in interpretation and use of the Specify data schema.  Additionally, with each version of Specify comes a new version of the data model.

Migration from legacy systems to either of these platforms requires mapping existing data schemas to that of Specify or Arctos. This process is similar to that of retrofitting a building: reworking structures to bring them up to code. This process is not without challenges. Some of them are viewed as an unavoidable consequence of bringing many different scientific domains into one unified system. For instance, participant CUVZ described the following challenge in migrating a lot-based collection (in which multiple specimens are preserved in one jar of alcohol) to Arctos, which is designed for individually cataloged records:
“Generally, [in] the marine collections and herpetology, there's like 50 specimens per jar. All 50 specimens have the same catalog number, versus something like mammals and birds where it's a one-to-one…. we had to itemize specimens within the record and give them an attribute per part so basically you'd have to put 50 different parts within the specimen record, so you could attribute sex, or measurement to that specific individual because they don't have a unique number.”
Thus, one aspect of retrofitting involves restructuring relationships between data tables.  In this case, a one to one relationship was reworked into a one-to-one schema.

Inevitably, there are fields that can’t be easily cross-walked between schemas.
One common way of working around schema mismatch in Specify is to “co-opt” fields for idiosyncratic purposes. Sometimes this is because Specify doesn’t have the field needed, but in other cases, it’s because some fields have technical constraints to them that conflict with local cataloging practices.  For instance, Adrain at U of Iowa had to co-opt a field to store catalog numbers because Specify 5 didn’t like the alpha’s in her alphanumerics:  
“With our specimens, we have suffixes, A B C D, so if you have a slab of rock with multiple specimens on it, the slab gets one catalog number, and then the specimens all have the same catalog number, but then they'll would have an A, B C or D added to it... so we couldn't have a letter in the unique number field in Specify, because it's just numbers, you couldn't have letters in there"

Predictably, co-opting fields, complicates later migrations. Karim at CU Boulder migrated her Invert Paleo database between 5 and 6; because so many fields had been co-opted, she had to compare field names in data entry forms “by hand” to the field names in the underlying schema to ensure that they will “map” properly to the new system: 
"whoever had set up the 5 databases had co-opted fields, so the field that was hard coded into the database of, like, "latitude" may not be where latitude was actually stored.  So when [the Specify team] went to do the mapping, they mapped latitude to latitude -- but it turned out latitude in 5 was in "text field 2" or something." 

Many of these co-opted fields appear to be, as Adrain at U of Iowa put it, “ghosts” of collections databases past.   In her case, a legacy dBase catalog that had been migrated to Specify 5 in the early 2000s, before she took over as CM. Despite the migration, she still regularly encounters inconsistencies or idiosyncrasies in the data that originated in the older system.  For instance:
-	The dBase system had a 1:1 relationship between specimens and citations; rather than changing the schema to accommodate specimens with multiple citations, the original CM created duplicate specimen records to add citation information to, with a pointer back to the original record.  These duplicate records were migrated to Specify and now need to be merged or eliminated.
The database has multiple fields for stratigraphy and locality because in dBase, “we only had a finite number of characters you could put in each field. So if there was detailed stratigraphy, and it didn't fit into one field… then there was another field, detailed stratigraphy 2, where you could carry on.... so we've got stuff that really should be in one field that's split into several fields."  




