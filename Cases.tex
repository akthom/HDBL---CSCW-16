\section{Results}

36 databases were described and managed by our participants (median per department = 3; see Appendix A for a full descriptions). Every collections database was, at the time of our interviews, in the process of being migrated, or were being prepared for a future migration\footnote{We did not specifically seek out departments undergoing database migrations.}. In addition to their work with collections databases, five participants also managed research databases, which they described as customized systems designed to integrate specimen and occurrence data for ecological studies. A third type of database mentioned by our participants were literature databases used to manage the museum's bibliographic references for taxonomic work. Research and literature databases may overlap or interlink with collections databases, but all three types are maintained as separate databases, often in MySQL or Access. \textit{N} of our participants managed transactional databases separate from their collections databases; these were typically used as bridges between the collections databases and Excel or Word for various “work tasks” (in one participant’s phrasing) such as processing loan documents or printing labels. In some cases (e.g. U of Iowa’s Paleontology collection, CU Boulder’s Invertebrate Zoology collection, and the LACM marine biology database) electronic databases have essentially replaced paper catalogs; in others (e.g. CU Boulder Invertebrate Paleontology, UT Austin) they are maintained concurrently with paper catalogs, which are viewed as being more stable and long-lasting.

\subsection{Laying the foundation: the evolution of collections databases}

Most of our participants were unsure when the collections databases had originally been digitized, and all but one of our participants (LACM) had to reverse engineer their collections databases to understand these origins. The collections databases described by our participants were typically undocumented and in varying states of denormalization when they were “inherited.” Consequently, extensive review of records one-by-one, coupled with conversations with senior staff, was necessary to understand the databases’ structure.  

While each database’s legacy structure was somewhat unique to its institution, we found evidence of what Brand called “vernacular” learning \cite{brand1995buildings}: regional styles of database design. For instance, the University of California’s Museum of Paleontology (UCMP) at Berkeley has long been a leader in museum standards development \cite[for example]{Star_1989}, but also a leader in NHM computing. The UCMP website was one of the first 100 websites online, and they were one of the first institutions to begin digitizing their collections catalogs. When the current collection manager at U of Oregon -- a UCMP alum -- first took over as custodian of that collections database, he discovered that the Oregon database had been "loosely based on the UCMP's database structure." It contained two tables -- one for specimens, and one for localities. However, the database’s original creator never actually saw the UCMP database:
\begin{quote}
“Ostensibly these two tables that would be linked through the locality number... there wasn't actually a link between the two tables in Access. So whoever had built it had... never actually gone through and done the job of connecting them."
\end{quote}

Similarly, the UT Austin collections database – also originally developed by a UCMP alum – features this two table, specimen/locality structure. Just as Brand found that buildings mimic one another’s facades over time, our participants report that NHM databases copy their structure from their peers. But crucially important in this mimetic process, the copied database had a loss of fidelity in the process - in the case of the UT Austin and Univeristy of Oregon,  the relations of the relational database were lost from the UCMP schema. We return to the implications of these partial copies in the discussion section.

\subsubsection{Remodeling \& Retrofitting: the emergence of, and migration to, Specify and Arctos}

With one exception, each of the collections databases described by our participants had been, or were in the process of being migrated to the community-developed database schemas of Arctos or Specify. Both of these databases are free of cost, and are designed specifically for use with NHM collections. Both Arctos and Speific also offer fairly extensive technical support and help with migration (time permitting, the Specify team will even normalize data to fit their schema). However, there are important distinctions between the two modelss: Arctos publishes its users’ collections on-line as a large, public, aggregated database; thus, stricter adherence to the Arctos data model is necessary. Specify, on the other hand, does not publish or aggregate data. This allows Specify users greater flexibility in interpretation and use of its data schema. 

Migration from legacy systems to either of these databases requires mapping existing data schemas to that of Specify or Arctos. This process is similar to that of retrofitting a building: reworking structures to bring them up to code. Some of the challenges to this process are the somewhat unavoidable consequence of bringing many different scientific domains into one unified system. For instance, the collection manager of the Boulder Vertebrate Zoology collection described the following challenge in migrating a lot-based collection (in which multiple specimens are preserved in one jar of alcohol) to Arctos, which is designed for individually cataloged records:
\begin{quote}
Generally, [in] the marine collections and herpetology, there's like 50 specimens per jar. All 50 specimens have the same catalog number, versus something like mammals and birds where it's a one-to-one... we had to itemize specimens within the record and give them an attribute per part so basically you'd have to put 50 different parts within the specimen record, so you could attribute sex, or measurement to that specific individual because they don't have a unique number.
\end{quote}

[Spend another sentence describing why this is important. If I'm following correctly, it mean that the previous collection database and the migrated version didn't allow same freedom of error in catalog numbers...? What is the consequence, other than inconience? Or what was innovative about his/ her solution to this problem?]

Thus, one aspect of retrofitting involves restructuring relationships between data tables.  In this case, a one to one relationship was reworked into a one-to-one schema.

\subsubsection{Loopholes or Building Code Violations?: "co-opting fields" and denormalization }

Inevitably, there are fields that cannot be easily cross-walked between the schema  of a collection database developed by a museum, and the standardized schemas of Specify and Arctos. One common way of working around schema mismatch in Specify is to “co-opt" pre-defined fields. Sometimes this is necessary because Specify does not have the field needed, but in other cases, it is because fields have technical constraints to them that conflict with local cataloging practices. For instance, the CM at the University of Iowa had to co-opt a field to store catalog numbers because Specify's schema objected to the alpha’s in her alphanumerics:
\begin{quote}
“With our specimens, we have suffixes, A, B, C, D, so if you have a slab of rock with multiple specimens on it, the slab gets one catalog number, and then the specimens all have the same catalog number, but then they'll would have an A, B, C, or D added to it... so we couldn't have a letter in the unique number field in Specify, because it's just numbers, you couldn't have letters in there"
\end{quote}
Co-opting fields may work in the short-term, but it complicates later migrations between versions of Specify. Over time, these co-opted fields behave, as the CM from Iowa put it, like “ghosts” of collections databases past -- in her case, a legacy catalog in the dBase schama that had been migrated to a version of Specify in the early 2000s, before she took over as CM. Despite the migration to Specify's schema, she still regularly encounters inconsistencies in the data that originated in the older system. As she explained: 
\begin{itemize}
\item the dBase system had a 1:1 relationship between specimens and citations; rather than changing the schema to accommodate specimens with multiple citations, a previous CM created duplicate specimen records to add citation information, with a pointer back to the original record. These duplicate records were migrated to Specify and now need to be merged or eliminated.
\item The database has multiple fields for stratigraphy and locality because in dBase, “we only had a finite number of characters you could put in each field. So if there was detailed stratigraphy, and it didn't fit into one field… then there was another field, detailed stratigraphy 2, where you could carry on.... so we've got stuff that really should be in one field that's split into several fields."  
\end{itemize}
Another participant (CU Boulder, Invertebrate Paleontology) reported similar "ghosts"; when she migrated her collections database between Specify 5 and 6, so many fields had been co-opted, she had to compare field names in data entry forms “by hand” to the field names in the underlying schema to ensure that they will “map” properly to the new system: 
\begin{quote}
"whoever had set up the 5 databases had co-opted fields, so the field that was hard coded into the database of, like, 'latitude' may not be where latitude was actually stored. So when [the Specify team] went to do the mapping, they mapped latitude to latitude -- but it turned out latitude in 5 was in "text field 2" or something." 
\end{quote}

[Take a sentence to explain the consequence of these examples]

In Specify, co-opting fields might be thought of as either a building code violation -- potentially dangerous misuse of a data schema -- or a helpful loophole that allows users to work within a community standard while still maintaining local curatorial practices. Fields in Arctos don't seem to be co-opted with such regularity, possibly because the Arctos managers are much stricter about adherence to their schema because the data is published online. Where Specify allows users to create independently governed structures, Arctos is more akin to a highly regulated planned community or a condominium complex: the contents of each unit may differ, but the structure is managed and maintained by an external board.

\subsubsection{Temporary housing: the persistence of Excel}

Several of our participants -- particularly, those managing database that had not yet been migrated to a system like Specify or Arctos -- utilize a different kind of loophole in the building code: deliberate denormalization. In some cases, this is a temporary step necessary to bringing data back “up to code”, so to speak, in which vocabularies are re-controlled and tables re-normalized through clustering and manipulation in flat-file programs like Excel and Open Refine. Some of this work simply can’t be done with existing database tools: SQL statements can’t parse irregularly formatted locality strings into atomized fields for county, state and city, or tell you whether a scientific name is misspelled or simply obscure or unfamiliar. Tools like Excel and Open Refine, on the other hand, allow users to drag and drop text; strip terminal spaces from strings; cluster and correct text; and further, as one participant noted, can be “just visually easier” to use.

In some cases, Excel becomes a longer term, staging zone between migrations. The Invertebrate Zoology CM at CU Boulder had such problems renormalizing her FileMaker database that she has instead exported the entire database into Excel for cleaning, and will be keeping it there until she’s able to finish cleaning the data for migration to Arctos. Another CM, at the Prairie Museum, is similarly using Excel as staging zone – but not as a flat file.  Rather, she is storing each table of her legacy database as a separate sheet within an Excel file, managing and updating relationships and primary keys by hand, until her museum’s IT department is able to set up a MySQL server for her use. 

[ a few more sentences wrapping this up here?]

\subsection{Vernaculars, Additions, Guest Houses \& Other Ways of Managing Concurrent Estates: developing and integrating research databases [this header needs embettering]} 

As noted above, several of our participants additionally manage research databases. Though these databases may include specimen data -- and in some instances, may have been initially drawn from aspects of collections databases (e.g. the NMITA and Mapstedi databases in our cases) -- they include additional data such as observational occurrences (e.g. the time, date and location at which a species was observed), ecological data and more, depending on the research they're meant to support. Where collections databases are fundamentally tied to the museums' whose collections they describe, NHM research databases are able to change hands and hosts over time, or represent . The research databases described by our participants were typically grant-funded (at least initially), the result of collaboration with other institutions, and aim to both integrate and improve upon disparate sources of natural history data. 

\subsubsection{Heading here previewing MioMap?}

One such integrative database is MioMap, which aims to aggregate specimen data to show how "major disruptions to the physical environment affected species richness, evolutionary patterns, and biogeographic patterns in mammals from approximately 30 million to 5 million years ago"\footnote{http://www.ucmp.berkeley.edu/miomap/about/index.html}. Its present manager, the CM at U of Oregon, originally adopted the project from his PhD advisor when completing his thesis at UCMP, continued work on it perfunctorily (along with several colleagues at other institutions) after graduating, and now is funded by an NSF project to "port" the database to Neotoma, another larger, also relational, community-developed paleoecology database. MioMap is also paralleled by FaunMap\footnote{http://www.ucmp.berkeley.edu/faunmap/} - which provides similar data on organisms from 40,000 to 500 years ago.

Though these three databases -- MioMap, FaunMap and Neotoma -- have all been developed separately, they have nevertheless learned form each other throughout their design. Yet, while they are being linked and integrated in some ways, their managers have been careful to mind certain property lines.  For instance, while FaunMap and MioMap can presently be queried through the same portal, and have the exact same relational structure, they are still separate databases.  Our participant offered two reasons for this, one scientific:
\begin{quote}
A lot of it has to do with the mindset that we have towards dates... once you're working in the Miocene, the uncertainty in the dates is much larger but not as problematic... Another thing that's important is that there are different groups of taxa, so you'll end up having experts on different ages of mammals, keeping the taxonomy up to date, and so, I guess that's not something that would necessarily be problematic if all the data were in a single database that stretches through the whole time... but having it broken up into constituent databases makes it easier for the Miocene workers to focus on the Miocene stuff...."
\end{quote}
And one social:
\begin{quote}
"A lot of it has to do with feelings of ownership and accessibility. There's a long history in paleontology of groups feeling like they don't want to have to share control of the data with other groups"
\end{quote}
Just as the UCMP collections database can be seen as having a stylistic influence on the structure of its peers', so do these research databases share the same vernacular to their structure -- while remaining within relevant property lines. Shared "styles" in database structure fosters collaboration; separate files keep neighbors from encroaching on one another's yards.

\subsubsection{Heading here previewing the Decapod database?}
Research databases may also include taxonomic databases: carefully curated collections of bibliographic references species descriptions. These may superficially seem like a simple bibliographic resource, but as our participant from LACM explains,
\begin{quote}
“It’s important to remember that for taxonomists, the bibliographic information are data. It's not just the description of where to find the reference. It's actually data that represent the publication date, the publication information for a particular taxon concept, and they are very, very careful about that information.”
\end{quote}
In his case, he helped create and now manages a publicly available database of Decapod (“ten-footed” crustaceans like lobsters and crabs) systematics literature. The database was initially developed for local use on an NSF grant, but “in typical fashion of many databases, this is the kind of thing that... just grew":
\begin{quote}
“We realized that we were amassing a large number of literature references to be the taxonomic references for the genus list that we were putting together. And simultaneously, we recognized that many of our collaborators who were trying to do the taxonomic work did not have access to all of the decapod literature. So, we simultaneously started amassing the… definitive version of the references for the publication, and for the co-workers on the project.”
\end{quote}
Unlike some of the collections databases described here, Pentcheff and his colleagues have been careful not to “improve” the data quality as records were input, because “the systematists involved in the project had a very strong insistence that the bibliographic data exactly reflect the publication as it existed... This was not an attempt to rectify the literature, it was an attempt to reflect the literature as it existed.” 

Preserving text verbatim presented its own challenges: students doing data entry often could not tell if a typo was intentional or not. Two provenance tracking mechanisms were added to the database: they required use of an annotation field to explain any change, any time to any record, and they now track and archive edits in a “pretty aggressive way”
The database is now fairly widely used by Decapod researchers (100 use it regularly, and a few thousand more casually), and Pentcheff credits the aggressive provenance-tracking and annotation with making it usable by other scholars:
\begin{quote}
“[When] we reflected with the data that we made available, the change history of the records, all of a sudden, it gave a huge amount of credibility because then they can see what level of trust they're willing to put on any individual record.... And we've had feedback from the community that having that, exposing that kind of metadata about the metadata [laughs] – the metadata metadata – is why they will very often tend to use our listing, rather than any other authoritative source where they are not able to check... the audit trail of the information.”
\end{quote}


[a few sentences explaining why this is relevant - am realizing now that this doesn't really cover the notion of "guest houses" i put in the heading. ]
[is part of it about showing that it's important to know what really matters - especially if what really matters changes over time. It tells you where you shower care and attention - and what you can let ride. Maybe like which rooms or parts of your house you fuss over and which are relatively neglected
[transition to discussion?]

