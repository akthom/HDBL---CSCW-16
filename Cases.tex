\section{Results}

36 databases were described and managed by our participants (median per department = 3; see Appendix A for a full descriptions). We sorted databases into the following types, which emerged from our analysis: collections management, research (including literature management) and transactional (e.g. processing loans, printing labels). All but one collections database was, at the time of our interviews, in the process of being migrated, or was being prepared for a future migration (n=29 collections databases total; 28 being migrated)\footnote{We did not specifically seek out departments undergoing collections database migration.}. In three cases (e.g. U of Iowa’s Paleontology collection, CU Boulder’s Invertebrate Zoology collection, and the LACM marine biology database) electronic collections databases have essentially replaced paper catalogs; in the rest they are maintained concurrently with paper catalogs, which are viewed as being more stable and long-lasting. Six research databases were additionally described by our participants: these include customized systems designed to integrate specimen and occurrence data for ecological studies, as well as literature databases used to manage bibliographic references for taxonomic work. While these may overlap or interlink with collections databases, they are maintained as separate databases, often in MySQL or Access. Finally, one transactional database was described by a participant at the U of Iowa, which is used as a bridge between the collections databases and Excel or Word for various “work tasks” such as processing loan documents or printing labels. 

Below, we report our findings as short vignettes, describing the evolution of collections and research databases. These vignettes are organized around architectural themes invoked from Brand, as a way of illustrating how they evolved over time, and how that evolution affected use.

\subsection{Unsteady foundations: the origin and evolution of collections databases}

Many of the collections databases we studied had unclear origins: their present managers were aware that they'd (obviously) been digitized from paper records at some point in the past, but were not sure when. Further, the hand-off of collections databases is rarely done face-to-face; there's often a gap between one collection manager retiring and another taking his or her place. All but one of these collections database were unnormalized to a degree, and lacked any supporting documentation when the current collection manager took over their care. Thus, many collections databases had to be reverse engineered by their current managers. This reverse engineering process requires extensive review of records, and conversations with senior staff simply to understand the databases’ structure. 

While the legacy structure of each database was somewhat unique to its institution, we found evidence of what Brand called \textit{vernacular building traditions}: regional or "folk" styles of construction \cite{brand1995buildings}. For instance, the collections database at the University of Oregon has been built in the same style as the collections database at the University of California’s Museum of Paleontology (UCMP): it's built around two main tables, one for specimens and one for localities. However, this style was not passed down through blueprints, but rather, through oral tradition. As the collection manager at the University of Oregon (a UCMP alum, as it happens) explains, the database’s original creator never actually saw the UCMP database. As a result:
\begin{quote}
“Ostensibly these two tables would be linked through the locality number... [but] there wasn't actually a link between the two tables in Access. So whoever had built it had... never actually gone through and done the job of connecting them."
\end{quote}
In other words, the Oregon database's creator had replicated the UCMP tables, but not replicated the relationships between the tables. As a result, the two tables were essentially maintained as two separate spreadsheets for years, and had no mechanisms in place to prevent duplicates, misspellings and other errors from being introduced. The database became extremely denormalized over time, and the current collection manager has had to spend many hours removing duplicate records and recontrolling vocabularies.

The UT Austin collections database – also originally built by a UCMP alum – similarly features this two table, specimen/locality structure, and has similarly lost the relationships between the tables over time. Thus, just as there are regional styles of architecture and building construction, so are there regional styles of database construction. And just as architectural features evolve and get lost over time, so does relational structure.

\subsubsection{Remodeling \& Retrofitting: the emergence of, and migration to, Specify and Arctos}
There are currently two primary community-developed databases being adopted within natural history museums: Arctos and Specify. Both of these databases are designed specifically for use with NHM collections. They are also both free of charge, are built around a pre-defined data schema which users cannot alter, and offer fairly extensive technical support for data migration and ingest (time permitting, the Specify development team will even clean and migrate data for its users). 

However, there are some important distinctions between the two programs: Arctos publishes its users’ collections online as a large, public, aggregated database; thus, stricter adherence to the Arctos data model is necessary. Specify, on the other hand, does not publish or aggregate data for its users, and thus (for better or for worse, as we discuss further below) allows users some greater flexibility in interpretation and use of the Specify data schema; however, it does change its data schema with each new version of the software, which forces its users to migrate their data somewhat more regularly.

All but one of the collections databases we studied had been, or were in the process of being migrated to Arctos or Specify. Migration from legacy systems to either of these databases requires mapping existing data schemas to that of Specify or Arctos. This process is similar to that of retrofitting a building: reworking structures to bring them up to new and safer building codes, or to install features that had not been available during their original construction. 

Mapping database schemas is a known - and difficult challenge. In NHM databases, some of these challenges are the somewhat unavoidable consequence of bringing many different scientific domains into one unified system. Different scientific domains have different ways of cataloging storing their specimens; insects are stored on individual pins, placed in drawers, and often cataloged in "lots" (groups of specimens) by the drawer; reptiles and fish are also often cataloged in lots, but are stored in jars of alcohol; mammals and birds are more likely to be cataloged and stored individually, and paleontological collections catalog each bone of an animal separately. Thus, migrating catalogs from one domain, with one way of numbering, to another, can be uniquely challenging. For instance, the collection manager of the Boulder Vertebrate Zoology collection described the following challenge in migrating a lot-based collection (in which multiple specimens are preserved in one jar of alcohol) to Arctos, which is designed for individually cataloged records:
\begin{quote}
“Generally, [in] the marine collections and herpetology, there's like 50 specimens per jar. All 50 specimens have the same catalog number, versus something like mammals and birds where it's a one-to-one... we had to itemize specimens within the record and give them an attribute per part so basically you'd have to put 50 different parts within the specimen record, so you could attribute sex, or measurement to that specific individual because they don't have a unique number.”
\end{quote}
Thus, one aspect of retrofitting involves restructuring relationships between data tables. In this case, a one to one relationship was reworked into a one-to-one schema.

\subsubsection{Loopholes in the building code: The "Co-opting fields" and denormalization }

During a database migration, many fields cannot be easily cross-walked between schemas. One common way of working around schema mismatch in Specify is to “co-opt" a field by using the predefined Specify field for something other than its intended datum. Sometimes this is necessary because Specify does not have the field needed, but in other cases, it is because fields have technical constraints to them that conflict with local cataloging practices. The collection manager at the University of Iowa explained that she co-opted a field to store catalog numbers because a new version of Specify objected to the alpha’s in her alphanumerics:
\begin{quote}
“With our specimens, we have suffixes, A, B, C, D, so if you have a slab of rock with multiple specimens on it, the slab gets one catalog number, and then the specimens all have the same catalog number, but then they'll would have an A, B, C, or D added to it... so we couldn't have a letter in the unique number field in Specify, because it's just numbers, you couldn't have letters in there"
\end{quote}

Co-opting fields can solve cross-walking dilemmas in the short-term, but it complicates later migrations between versions of Specify. The University of Iowa database shows one example of this: despite being migrated from dBase to Specify in the early 2000s, the database now still retains evidence of it's past structure. For instance, the database has multiple fields for stratigraphy and locality -- e.g. "stratigraphy 1," "stratigraphy 2" and so on -- because in dBase, “we only had a finite number of characters you could put in each field. So if there was detailed stratigraphy, and it didn't fit into one field.... then there was another field, detailed stratigraphy 2, where you could carry on.... so we've got stuff that really should be in one field that's split into several fields."
As a result, any query for stratigraphic data needs to be run over a range of fields, rather than just one. The collections manager describes these traces as "ghosts" that haunt her in her day-to-day work: quirks that can only be explained through a historical understanding of the database's past states, and that regularly get in the way of work with the database.

Another participant (CU Boulder, Invertebrate Paleontology) reported a similar haunting; when she migrated her collections database between Specify 5 and 6, so many fields had been co-opted, she had to compare field names in data entry forms “by hand” to the field names in the underlying schema to ensure that they will “map” properly to the new system: 
\begin{quote}
"whoever had set up the 5 databases had co-opted fields, so the field that was hard coded into the database of, like, 'latitude' may not be where latitude was actually stored. So when [the Specify team] went to do the mapping, they mapped latitude to latitude -- but it turned out latitude in 5 was in "text field 2" or something." 
\end{quote}
Co-opting the field in this case was done out of convenience; "text field 2" happened to appear on the form that the original user wanted to use for latitude, and so it was used. However, what worked in one version of Specify didn't work in the next.

In Specify, co-opting fields might be thought of as either a building code violation -- potentially dangerous misuse of a data schema -- or a helpful loophole that allows users to work within a community standard while still maintaining local curatorial practices. Fields in Arctos, on the other hand, can't be co-opted; Arctos database managers are much stricter about adherence to their schema because the data is published on-line. Where Specify allows users to create independently governed structures, Arctos is more akin to a highly regulated planned community: the contents of each building may differ, but the structure is managed and maintained by an external board.

\subsubsection{Temporary housing: the persistence of Excel}

Several collections databases -- particularly, those not yet migrated to a system like Specify or Arctos -- utilize a different kind of loophole in the building code: deliberate denormalization. Databases are migrated from a complex relational structure to a flat file like Excel. In some cases, this is a temporary step necessary to bringing data back “up to code”; for many, it's easier to clean and renormalize data in programs like Excel and Open Refine than it is a database. Some of this work simply can’t be done with existing database tools: SQL statements can’t parse irregularly formatted locality strings into atomized fields for county, state and city. Tools like Excel and Open Refine, on the other hand, allow users to drag and drop text; strip terminal spaces from strings; cluster and correct text; and further, as one collection manager noted, can be “just visually easier” to use.

In some cases, Excel becomes a longer term staging zone between migrations. The Invertebrate Zoology collections database was so denormalized and difficult to reverse engineer that the collection manager has instead exported the entire database into Excel for cleaning, and will be keeping it there until ready to migrate to Arctos. Another collections database, at the Prairie Museum, was so complex that its manager discovered it simply couldn't be migrated to Specify; there was simply no way to cross-walk the schemas. Each table of database is now being stored as a separate Excel file until it can be migrated to a MySQL instance. The collection manager is managing and updating relationships and primary keys by hand. 

\subsection{Vernaculars, Additions, Guest Houses \& Other Ways of Managing Concurrent Estates: developing and integrating research databases [this header needs embettering]} 

As noted above, several of our participants additionally manage research databases. Though these databases may include specimen data -- and in some instances, may have been initially drawn from aspects of collections databases (e.g. the NMITA and Mapstedi databases in our cases) -- they include additional data such as observational occurrences (e.g. the time, date and location at which a species was observed), ecological data and more, depending on the research they're meant to support. Where collections databases are fundamentally tied to the museums' whose collections they describe, NHM research databases are able to change hands and hosts over time. The research databases described by our participants were typically grant-funded (at least initially), the result of collaboration with other institutions, and aim to both integrate and improve upon disparate sources of natural history data. 

\subsubsection{MioMap Database}
One such integrative database is MioMap, which aims to aggregate specimen data to show how "major disruptions to the physical environment affected species richness, evolutionary patterns, and biogeographic patterns in mammals from approximately 30 million to 5 million years ago"\footnote{http://www.ucmp.berkeley.edu/miomap/about/index.html}. Its present manager, the CM at U of Oregon, originally adopted the project from his PhD advisor when completing his thesis work at UCMP. He continued work on it perfunctorily (along with several colleagues at other institutions) after graduating, and now is funded by an NSF project to port the database to Neotoma, another larger, also relational, community-developed paleoecology database. MioMap is also paralleled by FaunMap\footnote{http://www.ucmp.berkeley.edu/faunmap/} - which provides similar data on organisms from 40,000 to 500 years ago.

Though these three databases -- MioMap, FaunMap and Neotoma -- have all been developed separately, they have also learned form each other throughout their design. For instance, while FaunMap and MioMap can presently be queried through the same portal, and have the exact same relational structure, they are still separate databases.  Our participant offered two reasons for this, one scientific and one social. The scientific he explained as:
\begin{quote}
A lot of it has to do with the mindset that we have towards dates... once you're working in the Miocene, the uncertainty in the dates is much larger but not as problematic... Another thing that's important is that there are different groups of taxa, so you'll end up having experts on different ages of mammals, keeping the taxonomy up to date, and so, I guess that's not something that would necessarily be problematic if all the data were in a single database that stretches through the whole time... but having it broken up into constituent databases makes it easier for the Miocene workers to focus on the Miocene stuff...."
\end{quote}
And, the social reasons described as follows:
\begin{quote}
"A lot of it has to do with feelings of ownership and accessibility. There's a long history in paleontology of groups feeling like they don't want to have to share control of the data with other groups"
\end{quote}
Just as the UCMP collections database can be seen as having a stylistic influence on the structure of its peers', so do these research databases share the same vernacular to their structure -- while remaining within relevant property lines. Shared "styles" in database structure fosters collaboration; separate files keep neighbors from encroaching on one another's yards.

\subsubsection{Decapod database}
Research databases may also include taxonomic databases, or carefully curated collections of bibliographic references to species descriptions in published literature. These may superficially seem like a simple bibliographic resource, but as our participant from LACM explains,
\begin{quote}
It’s important to remember that for taxonomists, the bibliographic information are data. It's not just the description of where to find the reference. It's actually data that represent the publication date, the publication information for a particular taxon concept, and they are very, very careful about that information.
\end{quote}
In his case, he helped create and now manages a publicly available database of Decapod (“ten-footed” crustaceans like lobsters and crabs) systematics literature. The database was initially developed for local use on an NSF grant, but “in typical fashion of many databases, this is the kind of thing that... just grew." When asked to exlaborate on this pheonomon, he explained:
\begin{quote}
We realized that we were amassing a large number of literature references to be the taxonomic references for the genus list that we were putting together. And simultaneously, we recognized that many of our collaborators who were trying to do the taxonomic work did not have access to all of the decapod literature. So, we simultaneously started amassing the… definitive version of the references for the publication, and for the co-workers on the project.
\end{quote}
Unlike some of the collections databases described here, Pentcheff and his colleagues have been careful not to “improve” the data quality as records were input, because “the systematists involved in the project had a very strong insistence that the bibliographic data exactly reflect the publication as it existed... This was not an attempt to rectify the literature, it was an attempt to reflect the literature as it existed.” 

Preserving text verbatim presented its own challenges: students doing data entry often could not tell if a typo was intentional or not. Two provenance tracking mechanisms were added to the database: they required use of an annotation field to explain any change, any time to any record, and they now track and archive edits in a “pretty aggressive way”. The database is now fairly widely used by Decapod researchers (100 use it regularly, and a few thousand more casually), and Pentcheff credits the aggressive provenance-tracking and annotation with making it usable by other scholars:
\begin{quote}
“[When] we reflected with the data that we made available, the change history of the records, all of a sudden, it gave a huge amount of credibility because then they can see what level of trust they're willing to put on any individual record.... And we've had feedback from the community that having that, exposing that kind of metadata about the metadata [laughs] – the metadata metadata – is why they will very often tend to use our listing, rather than any other authoritative source where they are not able to check... the audit trail of the information.”
\end{quote}


[a few sentences explaining why this is relevant - am realizing now that this doesn't really cover the notion of "guest houses" i put in the heading. ]
[is part of it about showing that it's important to know what really matters - especially if what really matters changes over time. It tells you where you shower care and attention - and what you can let ride. Maybe like which rooms or parts of your house you fuss over and which are relatively neglected
[transition to discussion?]

