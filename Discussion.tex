\section{Discussion}

Like buildings, databases adapt and change over time in response to their users -- and those constraints and changes at the logical level, in turn, can impact ways and styles of work.  Here we discuss in further depth the implications of this co-shaping of work for CSCW, particularly as it affects work and information systems in the very long term. We identify a number of implications not just for CSCW research and design, but also for that of data curation and digital preservation systems, and argue that a CSCW perspective could greatly improve the quality of the latter. [come back to this]

\subsection{Schemas, software, and the co-shaping of work}

Logical models impact the work of this group in several key ways: 
\begin{enumerate}
\item The kind and format of data that can be stored in the database is fundamentally shaped by the logical model. When data need to be entered that don’t “fit” into this model, work-arounds are used, such as putting data into notes or remarks fields, co-opting fields, or exporting the database to a flat file for curation or temporary storage (at least, temporary in theory). Thus, databases become “misshapen” at the edges of their schemas: the points at which the original designer decided to “end” his view of the world. [this clearly relates to star – cite here?] While these work-arounds may suffice as short-term solutions for workers at the time of their initial implementation, they create problems down the road, when databases need to be migrated either to ward off obsolescence or keep up with new technologies and community needs. Thus, its important to begin thinking of logical models as crafted iteratively, over time, and that change through use. Logical schemas can be fairly malleable over time – provided the database manager has the tools necessary for reshaping. Some of this is innate in Codd’s model; the relational model is, after all, fundamentally intended to support addition of new tables, fields and relationships over time. 
\item Complex relationships between tables – particularly during a “handover” between managers and particularly when they are not well documented – can act as a barrier to use. Some of this is due to the innate difficulty of reverse engineering an unfamiliar system, but it may also be because logical schemas not only represent a particular way of thinking of data – but also, a way of working with data. [Example]
\item Databases in general, and logical models in specific, demand an administrator. As we reviewed at the beginning of this paper, Codd’s relational model is dependent on an assumption that people using databases will fall into one of two groups: users and administrators. In NHMs, though, the collections managers are typically both. [but isn't that a good thing? they know what the users  want because they are users too. They just may not be that expert in db design]

\end{enumerate}

working in still:
 While Specify and Arctos users are the administrators of their collections, and collections’ data, they are not the administrators of her database's underlying schema, per se: the Specify and Arctos developers are.  While the Specify database, in particular, is certainly "learning" from its user base in some important ways – making changes to schemas in response to feedback   


That said, 

However, the method through which these new features can or are added is often somewhat idiosyncratic and warrants further study, particularly from a practical or CSCW perspective. For instance, during the “handover” stage, when a new collection manager is coming to “learn” the collections database she is charged with, CMs often have to “blow apart” databases (as a CM at Austin put it) into individual tables to clean data. The frequent use of Excel as a staging ground or curatorial tool implies that some level of transformation to a flat file may be necessary for not just migration between systems, but transfer between database managers. 


Well-managed database don’t show traces of use or change over time.  A lot of curatorial work is about removing evidence of a database’s learning process: erasing the fingerprints off the catalog cards, in a way.  Part of this is probably related to biodiversity’s increasingly informatic way of knowing: data creators are striving to make their data more and more machine-readable (and often in very subtle ways: for instance, humans aren’t nearly as confused by trailing spaces as Davis’ database was). 

Part of the challenge of this data “cleaning” for community informatic use: community resources, community built databases, often rely on community standards and data models.  Briding the differences between even closely related disciplines (e.g. fish vs. reptiles) can result in a ‘lossy’ database: one in which data are “shaved” to fit into a form, rather than one in which forms were made for data.  
Application profiles can’t necessarily fix this – when people don’t just store different kinds of data, but rather, store the same data in a different way.

Of the people using flat files in Access, they seem to be keeping things in Access instead of, say, Excel, for the functionality offered for printing forms and reports. 


On collaboration: capacity for cooperative work tends to contract during migrations – but particularly for migrations with unexpectedly long durations.


\subsection{Comparisons with related studies}
[this might need moving or deleting, depending on space]
\subsubsection{Voida et al}

Voida et al.'s study of coordinators of volunteers at various non-profit organizations \cite{voida2011homebrew} has interesting similarities and differences from our study of NHMs. They found considerable use of other software (especially personal office applications such Excel and Outlook) to record information that might be expected to be more appropriately stored in a database. We also found considerable Excel use but not Outlook. The difference may be accounted for by one of the main activities of the volunteer coordinators being to communicate with volunteers. 

By contrast, collections managers typically had much more database expertise and comfort, and yet at times they too used spreadsheets to store data. We believe this is an important point of comparison. It can be tempting to say that some people use spreadsheets to store data rather than a proper database because they don't know any better or lack the skills to develop and maintain a database. This was not the case for our collections managers and so we must seek other reasons for their use (at times) of spreadsheets.

Clearly for processing certain kinds of result, spreadsheets are an entirely appropriate tool. It is not surprising that scientists would import data from a database into a spreadsheet in order to run certain calculations. But spreadsheets get used for many other purposes – just as email systems do \cite{bellotti2005quality}. Reasons include familiarity and facility. If you use spreadsheets a lot, then through inertia it is just there, already running on you computer. Also through regular use you learn how to be able to do many things with them, you are comfortable doing them, and you may also be more comfortable innovating and tinkering with them.  Consequently spreadsheets may be a convenient location for both mundane and more experimental data use. These include data management activities such as data cleaning and consistency checking.  Other reasons include the relative visibility (and comforting familiarity) of the tabular representation and the ease of checking the consequences of actions \cite{nardi1991twinkling}. As a conceptually comfortable data cache (or staging zone), there can be relatively few concerns about spreadsheet use. However, as spreadsheets start to become slightly longer term repositories of data, or serve as a resource parallel to but unsynchronized with the database but  we might worry more. A buildings analogy might be a room that 'temporarily' becomes an additional storage space, but then never reverts to its main use.

A telling perspective from the NPO study is that “information management is not the real work of volunteer coordination; it is overhead.” This naturally affects the allocation of effort, resources and indeed enthusiasm. For collections managers, information management is far more central. But if we see similarities in activity between these different settings despite the contextual differences it requires us to seek better explanations of why people use the technologies the way that they do. From the NPO study: “In lieu of a system that can do everything, volunteer coordinators continually reconfigure their homebrew databases—swapping one system for another and hoping the new set of systems will help reduce overhead in managing information.”  and: “We heard over and over again that volunteer coordinators were in the process of migrating their data from one application to another.” These migrations seem more frequent and perhaps more ad hoc than in our natural history museum settings, but that migrations are a recurrent issue in both contexts is important to bear in mind, and rises the question of how common migrations are in yet other contexts. If you are aware that your database is likely to be migrated a number of times in its life, this may (or perhaps should) have an impact in how you design and maintain it. The act of migration is often problematic. In the case of the NPOs: "Existing data either has to be ported — frequently necessitating manual re-entry of the data or selective copying and pasting — or abandoned. New systems rarely, if ever, encompass the same set of features or afford the same degree of flexibility as previous systems. Changes in the information managed by one application influence information management in others.” Despite better database skills and a closer alignment of database management with the 'real work' of the organization, certain similarities of re-entry and particularly of checking occurred in museums.

One challenge is the evolving purpose and use of a database. How can a database achieve, in Ribes’ terms, ‘technoscientific flexibility’? \cite{ribes2014kernel} . That requires the database to be able to respond to changing user needs and to the opportunities of new technologies. Migration plays a role in both of these.

Another important piece of work that we want to compare our findings with involves a collaborative project between database researchers and biologists   \cite{jagadish2007making}. This uncovered a number of database usability issues that can arise when domain experts (but who are not necessarily database experts) have to deal with the development and use of databases as part of their work. In general terms they note: “When we speak of usability, we mean much more than just the user interface, which is only a part of the usability equation. A more fundamental concern is that of the underlying architecture.” We too want to uncover the way that the underlying architecture affects use, and in particular changing use over time. As database expects they also note an irony in normalization. Codd proposed normalization as a way to protect end users from the underlying structure of the data, and make that data more robust over time and incremental changes. Jagadish et al. note that “We break apart information during the database design phase such that everything is normalized—space efficient and updatable. However, the users will have to stitch the information back together to answer most of the real queries. The fundamental issue is that joins destroy the connections between information pertaining to the same real world entities and are nonintuitive to most normal users." As a result, databases may fail to be normalized for reasons of usability - or even deliberately de-normalized. [any examples, riffs on these ideas from our data?] Finally, spreadsheets as data containers appear yet again - but with a usability perspective on why: “While joins across multiple normalized tables may be difficult, people are certainly used to seeing data represented in simple two-dimensional tables. The popularity of the Excel spreadsheet as a data model speaks to this. For situations where data can be represented conveniently as a table, a tabular model is certainly appropriate."

\subsubsection{Bietz and Lee}

They talk about participants idealizing a "Perfect Database" - ours do too, to an extent, but it's more collections based.

\subsection{Temporally distributed cooperative work}

A consideration of database use, change and maintenance over long periods of time has numerous connnections to CSCW. Handovers and handoffs are a recurrent theme in CSCW in a variety of settings ranging from medicine to paper mills (e.g. \cite{sarcevic2009information};  \cite{auramaki1996paperwork}). In the case of NHM databases a critical issue for a database to be successfully long-lived is the handover from one database manager to another. These occur at much longer time periods than the shift-level handover (from years to decades) and so are likely to a far less practiced or routinized skill. Ideally a handover involves a period of time for face to face meetings, even an apprenticing into the role for the newcomer. But it can also be more indirect, involving the use of documents and a degree of reverse engineering. In problematic cases it is a kind of CSCW where there is far less collaboration than is desired, and certain computational resources compensate for the lack of collaboration. 

Given the long term use of the database, there may even be work to hand over information to your future self – sending yourself messages to be read many years later that will facilitate the maintenance, interpretation or migration of the data. Such issues are not unique to databases by any means. They are strongly analogous to work in the ongoing documenting (or lack thereof) and maintenance of long lived programs – a topic studied by software engineering for decades. It is rather odd that equal attention has not been paid to the very similar but not identical challenges of maintaining databases as opposed to programs over many years. Both handovers and migrations have something of a rhythmic aspect, but they are are more like the bursty rhythms of Jackson et al. \cite{jackson2011collaborative}

\subsubsection{Infrastructure, collaboration and levels of visibility}

As Star reminds us, infrastructure becomes visible upon breakdown. What we would add is that breakdown is often when collaboration also increases, - or maybe the infrastructure-enabling collaboration suddenly becomes more visible to some of the people involved. 

[I don't think that a lot of what was in this section bears with the data - and even if it does, this section may need to be entirely scrapped - because we don't foreshadow this discussion.  OR - need to be clearer that we're talking about either:
a) unseen collaboration between present users and past users, or 
b) cooperative work btwn users and designers of databases like Specify.

I'd also written in my field exam on steps people take to deliberately bring infrastructure into view - that might fit but I am not sure]

\subsubsection{The the mundane, but meaningful}

Our focus is on long-lived scientific databases in natural history museums where new entries are added, and certain entries and values may be revised over time. The databases are used by scientists and are expected to continue to be used for years, decades and centuries – just as their precursors have. These are not particularly large databases - they would not count as "big data" - nor do they contain rapidly changing transactional data. They are not databases whose contents are now fixed and are not currently being used, but need to be archived in case they are  needed in the future, as explored by Olson \cite{olson2010database}. This last case of archiving is less like one of Brand’s learning buildings and more like an historic monument with a restrictive preservation order applied to it.

Although not large from the perspective of giant astronomy projects or corporate examples, they are  significant and are not trivially small. Many can reside in current powerful laptop computers, even it it would be better to have them on servers. They are unlikely to receive the attention of database researchers (Jagadish is a commendable exception) who are likely to be working on the challenges of much greater scale, levels of use and rates of change. But improving their usability for those who create, update and use them, and knowing the degree of difference (if any) between users and maintainers is important. In particular the collaborations around their use and particularly around their maintenance over decades (and, we can hop, centuries) is an important challenge where insights from CSCW can play a vital role.

We suspect that there are similar database uses and needs outside Natural History Museums, and not just in other scientific settings. Inspired by Voida et al. we think there may be  many other domains ( where people “need to manage information too complex for paper or personal office applications, but who cannot confront the overhead of using enterprise “solutions.” We would go further and note that with increasingly powerful machines there is an opportunity and even an expectation that people will be managing ‘moderate sized’ datasets. These are typically too large or unwieldy for paper or personal office applications, but look like very modest datasets from the perspectives of many systems administrators or especially database researchers. Our laptops and PCs may run them well, but we may not. The tools, features and interfaces needed for moderate databases are different from those for large ones. As Jagadish et al. remind us, too much power and many choices can be a significant disadvantage in moderate database usability.

\subsubsection{Design recommendations/future work?}

\textbf{design: }
- how do we design relational databases that support not just information storage and retrieval, but rather,  curatorial work and collections management? e.g. things like georeferencing; taxonomic referencing; clustering and batch editing as in Open Refine. (or: how can we merge open refine with specify?)
- Like Buneman, we see a definite need to support better provenance tracking in curated databases \cite{Buneman_2006}; however, we would argue that the actions tracked or bundled at at a higher level than "insert, delete, copy, and paste" actions -- while the core of data curatorial work may appear to a computer like a sequence of insertions, deletions and copies
- additionally: need to support visual rearrangement and browsing of tables and tuples; this might have SQL at its core but needs to be more user friendly. Bulk of HCI/UX studies have been about query construction - less has been written about user-friendliness of database GUIs, particularly from a curatorial perspective (e.g. someone doing quality assessment, editing, as opposed to just data entry, or just information retrieval
- need support for review of students'/volunteers' data entry  -- right now there's no great way to handle this.

\textbf{future work:}
- logical modeling as a design problem for CSCW
- in what ways do people come to know an information system? we need to study not just long lived databases but also people who work with databases for a longer period of time - people who gain almost tactile knowledge of information systems based on consistent use (as opposed to studies of how naive users use a database once).
- We'd like to study comprable organizations with relational databases.