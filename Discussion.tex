\section{Discussion}

Like buildings, databases adapt and change over time in response to their users -- and those constraints and changes at the logical level, in turn, can impact ways and styles of work.  Here we discuss in further depth the implications of this co-shaping of work for CSCW, particularly as it affects work and information systems in the very long term. We identify a number of implications not just for CSCW research and design, but also for that of data curation and digital preservation systems, and argue that a CSCW perspective could greatly improve the quality of the latter. [needs to be revised]

\subsection{Temporally dispersed cooperative work}

The CSCW community has long studied how CSCW is done in contexts of great spatial or geographic dispersion; here, though, we contribute to recent work exploring how CSCW is done across dispersed \textit{temporal} distances (REFS - Jackson papers, lindley). However, where several studies have explored how time matters in fast-paced, time-sensitive environments such as nursing \cite{sarcevic2009information, Reddy_2006}  and paper mills \cite{auramaki1996paperwork}, ours contributes to an understanding of how time matters in extremely long-lived, if slower-paced environments: memory institutions. Museums must, by dint of their preservation mandate, take an extremely long view of work; as Bowker has noted, the process of databasing the world is a fundamentally a long-term one, and one that makes most of its progress through slow, steady curatorial work. However, this work isn't without disruption; we selected databases as our unit of analysis because they often have longer tenures at museums than their curators! Our cases show that for NHMs, the most critical point of information transfer isn't at a shift change, as it is in medical work; rather it's the point at which one collections manager leaves, and the next comes in to take over care of both the collection and the collection databases. These are information handovers at the scale of generations.

Reframing this process as temporally dispersed cooperative work helps us identify areas in need of design interventions, particularly given that a specific technical artifact -- the database -- is being used for the handover. In our cases, we see that despite these being curated databases, they are not databases that particularly support curatorial work: they lack much of the functionality that collection managers need to re-control vocabularies after years of use (or misuse), or review data entered by volunteers, or reconcile their data with external sources, thus leading to the continued use of programs like Excel. A long-view of cooperative work shows us that there's a need to design relational databases that support not just information storage and retrieval, but rather, data curation as well, so that data managers can assess changes to data and schemas that accrue over time, and make changes to the database's structure or contents as needed. Additionally, like \cite{Buneman_2006} and [JAGADISH], we see a definite need to support better provenance tracking in curated databases; however our cases contribute to an understanding what provenance information is needed at a higher level than even the "course-grained" provenance they describe. Our cases echo Voida, Harmon & Al Ani's finding that there's a need to not just track provenance within one system, but between data systems (e.g. between Excel and databases).

\subsection{Schemas, software, and the co-shaping of work}

One of the motivating ideas behind this work was that databases shape people just as much as people shape databases; that is, that databases can structure how people do work. In our cases, work with databases was co-shaped by people, data models, and software in several key ways: 
\begin{enumerate}
\item The kind and format of data that can be stored in a database are fundamentally shaped by not just the data model, but also the user’s ability to change that model. Where Codd may have intended that the logical schema be updated to accommodate new data, ad hoc work-arounds (putting data into notes or remarks fields, co-opting fields, or exporting the database to a flat file for curation) are often used when users cannot change the schema due to other constraints. These work-arounds may work reasonably well at the time of their initial implementation, but can have unexpected effects when databases need to be migrated either to ward off obsolescence or meet evolving community needs (as illustrated by the problems encountered by Specify users in migrating co-opted fields). 
\item Even when databases are initially well-normalized and built “up to code”, complex relationships between tables can eventually become a barrier to use – particularly if there is a mismatch between the original creator’s ability to manipulate the data model and the user’s or new custodian’s. In our cases, this barrier became most obvious during the “handover” phase, in which a new collections manager takes over custody of a database and must painstakingly reverse engineer its structure. 
\item That said, complex relationships can also facilitate use -- when they’re being managed by someone who is able to act as a true administrator of the database. For instance, the Decapod Systematics Database described above features a fairly complex relational structure designed to track changes, as well as make visible the reasons for those changes. Thanks to several well-implemented web forms, these relationships do not need to be understood by the users of the database – only by the administrator.
\item Databases learn their “vernacular” from their “neighbors” just as buildings do; in our cases, “neighbors” include colleagues within the same community of practice who share their data structures, and encourage one another to adopt similar designs for later integration. The NHM community’s move toward use of Specify and Arctos may also be viewed as a number of databases adopting a common style. 
\end{enumerate}

Finally, the broad adoption of Specify and Arctos by NHMs can be viewed as the result of their collections databases “demanding” more formal administrators. As we reviewed at the beginning of this paper, Codd’s relational model is rooted in an assumed division of labor between administrators and users: users do data entry and retrieval, and administrators manage the mappings between the physical, logical, and conceptual levels of the database. However, in our cases, users and administrators are often one in the same. Because both Specify and Arctos come with pre-designed, unalterable data schemas, their use essentially outsources some of that administrator role to the Specify and Arctos developers.  [come back to this, try to integrate with the next paragraph]

While Specify and Arctos users are the administrators of their collections, and collections’ data, they are not the administrators of her database's underlying schema, per se: the Specify and Arctos developers are.  While the Specify database, in particular, is certainly "learning" from its user base in some important ways – making changes to schemas in response to feedback – the learning process is somewhat more delayed than it would be if local 


\subsubsection{The the mundane, but meaningful: towards a typology databases}

[andrea working on this first thing in morning]

- this study has looked at medium sized databases (I really want to talk about this without having to say big data).

- several types of databases have been mentioned in our study
- we expect these types may translate to other domains, and this list might be helpfully expanded
- implications for design of not just datbaases, but also database teaching materials (not everyone's building a parts database!)

Our focus is on long-lived scientific databases in natural history museums where new entries are added, and certain entries and values may be revised over time. The databases are used by scientists and are expected to continue to be used for years, decades and centuries – just as their precursors have. These are not particularly large databases - they would not count as "big data" - nor do they contain rapidly changing transactional data. They are not databases whose contents are now fixed and are not currently being used, but need to be archived in case they are  needed in the future, as explored by Olson \cite{olson2010database}. This last case of archiving is less like one of Brand’s learning buildings and more like an historic monument with a restrictive preservation order applied to it.

Although not large from the perspective of giant astronomy projects or corporate examples, they are  significant and are not trivially small. Many can reside in current powerful laptop computers, even it it would be better to have them on servers. They are unlikely to receive the attention of database researchers (Jagadish is a commendable exception) who are likely to be working on the challenges of much greater scale, levels of use and rates of change. But improving their usability for those who create, update and use them, and knowing the degree of difference (if any) between users and maintainers is important. In particular the collaborations around their use and particularly around their maintenance over decades (and, we can hop, centuries) is an important challenge where insights from CSCW can play a vital role.

We suspect that there are similar database uses and needs outside Natural History Museums, and not just in other scientific settings. Inspired by Voida et al. we think there may be  many other domains ( where people “need to manage information too complex for paper or personal office applications, but who cannot confront the overhead of using enterprise “solutions.” We would go further and note that with increasingly powerful machines there is an opportunity and even an expectation that people will be managing ‘moderate sized’ datasets. These are typically too large or unwieldy for paper or personal office applications, but look like very modest datasets from the perspectives of many systems administrators or especially database researchers. Our laptops and PCs may run them well, but we may not. The tools, features and interfaces needed for moderate databases are different from those for large ones. As Jagadish et al. remind us, too much power and many choices can be a significant disadvantage in moderate database usability.

\subsubsection{Design recommendations/future work?}

\textbf{design: }
- how do we design relational databases that support not just information storage and retrieval, but rather,  curatorial work and collections management? e.g. things like georeferencing; taxonomic referencing; clustering and batch editing as in Open Refine. (or: how can we merge open refine with specify?)
- Like Buneman, we see a definite need to support better provenance tracking in curated databases \cite{Buneman_2006}; however, we would argue that the actions tracked or bundled at at a higher level than "insert, delete, copy, and paste" actions -- while the core of data curatorial work may appear to a computer like a sequence of insertions, deletions and copies
- additionally: need to support visual rearrangement and browsing of tables and tuples; this might have SQL at its core but needs to be more user friendly. Bulk of HCI/UX studies have been about query construction - less has been written about user-friendliness of database GUIs, particularly from a curatorial perspective (e.g. someone doing quality assessment, editing, as opposed to just data entry, or just information retrieval
- need support for review of students'/volunteers' data entry  -- right now there's no great way to handle this.

\textbf{future work:}
- logical modeling as a design problem for CSCW
- in what ways do people come to know an information system? we need to study not just long lived databases but also people who work with databases for a longer period of time - people who gain almost tactile knowledge of information systems based on consistent use (as opposed to studies of how naive users use a database once).
- We'd like to study comprable organizations with relational databases.