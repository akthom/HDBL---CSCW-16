\section{Discussion}

Like buildings, databases adapt and change over time in response to their users -- and those constraints and changes at the logical level, in turn, can impact ways and styles of work.  Here we discuss in further depth the implications of this co-shaping of work for CSCW, particularly as it affects work and information systems in the very long term. We identify a number of implications not just for CSCW research and design, but also for that of data curation and digital preservation systems, and argue that a CSCW perspective could greatly improve the quality of the latter. [come back to this]

\subsection{Temporally dispersed cooperative work}

The CSCW community has long studied how CSCW is done in contexts of great spatial or geographic dispersion; here, though, we contribute to recent work exploring how CSCW is done across dispersed \textit{temporal} distances and diverse rhythms of work (REFS - Jackson papers, lindley). However, where several studies have explored how time matters in fast-paced, time-sensitive environments such as nursing \cite{sarcevic2009information, Reddy_2006}  and paper mills \cite{auramaki1996paperwork}, ours contributes to an understanding of how time matters in extremely long-lived, if slower-paced environments: information handovers at the scale of generations. Museums must, by dint of their preservation mandate, take an extremely long view of work; as Bowker has noted, the process of databasing the world is a fundamentally a long-term one, and as Thomer et al have noted, it is one that has made the most of its progress through slow, steady curatorial work [I want to reference a talk I gave about my documents to datasets paper - is this dumb? Should i just reference the paper even though the curatorial bit didn't make it in? Or not reference anything?]. Thus, for NHMs, the most critical point of information transfer isn't at a shift change as it is in medical work; rather it's the point at which one collections manager leaves, and the next comes in to take over care of both the collection and the collection databases. Where information transfer at a shift change may require quick summary of the day's work, informaiton transfer at a generational scale requires documentation of organizational memory, and provenance of data systems that stretches years.  

Further, where Saravecic and Burd note that "information transfer under time pressure and stress often leads to information loss," our work shows that temporal slack or easiness or laxity [am trying to name the opposite of pressure] -- can lead to information loss as well. Furthermore: many of our database's custodians expressed feelings of not having enough time to care for their databases; this leads us to conclude that work environments with long


In the case of NHM databases a critical issue for a database to be successfully long-lived is the handover from one database manager to another. These occur at much longer time periods than the shift-level handover (from years to decades) and so are likely to a far less practiced or routinized skill. Ideally a handover involves a period of time for face to face meetings, even an apprenticing into the role for the newcomer. But it can also be more indirect, involving the use of documents and a degree of reverse engineering. In problematic cases it is a kind of CSCW where there is far less collaboration than is desired, and certain computational resources compensate for the lack of collaboration. 

Given the long term use of the database, there may even be work to hand over information to your future self – sending yourself messages to be read many years later that will facilitate the maintenance, interpretation or migration of the data. Such issues are not unique to databases by any means. They are strongly analogous to work in the ongoing documenting (or lack thereof) and maintenance of long lived programs – a topic studied by software engineering for decades. It is rather odd that equal attention has not been paid to the very similar but not identical challenges of maintaining databases as opposed to programs over many years. Both handovers and migrations have something of a rhythmic aspect, but they are are more like the bursty rhythms of Jackson et al. \cite{jackson2011collaborative}

\subsection{Schemas, software, and the co-construction of work}

In our cases, work with databases was co-constructed by people, data models, and software in several key ways: 
\begin{enumerate}
\item The kind and format of data that can be stored in a database are fundamentally shaped by not just the data model, but also the user’s ability to change that model. Where Codd may have intended that the logical schema be updated to accommodate new data, ad hoc work-arounds (putting data into notes or remarks fields, co-opting fields, or exporting the database to a flat file for curation) are often used when users cannot change the schema due to other constraints. These work-arounds may work reasonably well at the time of their initial implementation, but can have unexpected effects when databases need to be migrated either to ward off obsolescence or meet evolving community needs (as illustrated by the problems encountered by Specify users in migrating co-opted fields). 

\item Even when databases are initially well-normalized and built “up to code”, complex relationships between tables can eventually become a barrier to use – particularly if there is a mismatch between the original creator’s ability to manipulate the data model and the user’s or new custodian’s. In our cases, this barrier became most obvious during the “handover” phase, in which a new collections manager takes over custody of a database and must painstakingly reverse engineer its structure. 
\item That said, complex relationships can also facilitate use -- when they’re being managed by someone who is able to act as a true administrator of the database. For instance, the Decapod Systematics Database described above features a fairly complex relational structure designed to track changes, as well as make visible the reasons for those changes. Thanks to several well-implemented web forms, these relationships do not need to be understood by the users of the database – only by the administrator.
\item Databases learn their “vernacular” from their “neighbors” just as buildings do; in our cases, “neighbors” include colleagues within the same community of practice who share their data structures, and encourage one another to adopt similar designs for later integration. The NHM community’s move toward use of Specify and Arctos may also be viewed as a number of databases adopting a common style. 
\end{enumerate}

Finally, the broad adoption of Specify and Arctos by NHMs can be viewed as the result of their collections databases “demanding” more formal administrators. As we reviewed at the beginning of this paper, Codd’s relational model is rooted in an assumed division of labor between administrators and users: users do data entry and retrieval, and administrators manage the mappings between the physical, logical, and conceptual levels of the database. However, in our cases, users and administrators are often one in the same. Because both Specify and Arctos come with pre-designed, unalterable data schemas, their use essentially outsources some of that administrator role to the Specify and Arctos developers.  [come back to this]

While Specify and Arctos users are the administrators of their collections, and collections’ data, they are not the administrators of her database's underlying schema, per se: the Specify and Arctos developers are.  While the Specify database, in particular, is certainly "learning" from its user base in some important ways – making changes to schemas in response to feedback – the learning process is somewhat more delayed than it would be if local 


\subsubsection{The the mundane, but meaningful: towards a typology databases}

Our focus is on long-lived scientific databases in natural history museums where new entries are added, and certain entries and values may be revised over time. The databases are used by scientists and are expected to continue to be used for years, decades and centuries – just as their precursors have. These are not particularly large databases - they would not count as "big data" - nor do they contain rapidly changing transactional data. They are not databases whose contents are now fixed and are not currently being used, but need to be archived in case they are  needed in the future, as explored by Olson \cite{olson2010database}. This last case of archiving is less like one of Brand’s learning buildings and more like an historic monument with a restrictive preservation order applied to it.

Although not large from the perspective of giant astronomy projects or corporate examples, they are  significant and are not trivially small. Many can reside in current powerful laptop computers, even it it would be better to have them on servers. They are unlikely to receive the attention of database researchers (Jagadish is a commendable exception) who are likely to be working on the challenges of much greater scale, levels of use and rates of change. But improving their usability for those who create, update and use them, and knowing the degree of difference (if any) between users and maintainers is important. In particular the collaborations around their use and particularly around their maintenance over decades (and, we can hop, centuries) is an important challenge where insights from CSCW can play a vital role.

We suspect that there are similar database uses and needs outside Natural History Museums, and not just in other scientific settings. Inspired by Voida et al. we think there may be  many other domains ( where people “need to manage information too complex for paper or personal office applications, but who cannot confront the overhead of using enterprise “solutions.” We would go further and note that with increasingly powerful machines there is an opportunity and even an expectation that people will be managing ‘moderate sized’ datasets. These are typically too large or unwieldy for paper or personal office applications, but look like very modest datasets from the perspectives of many systems administrators or especially database researchers. Our laptops and PCs may run them well, but we may not. The tools, features and interfaces needed for moderate databases are different from those for large ones. As Jagadish et al. remind us, too much power and many choices can be a significant disadvantage in moderate database usability.

\subsubsection{Design recommendations/future work?}

\textbf{design: }
- how do we design relational databases that support not just information storage and retrieval, but rather,  curatorial work and collections management? e.g. things like georeferencing; taxonomic referencing; clustering and batch editing as in Open Refine. (or: how can we merge open refine with specify?)
- Like Buneman, we see a definite need to support better provenance tracking in curated databases \cite{Buneman_2006}; however, we would argue that the actions tracked or bundled at at a higher level than "insert, delete, copy, and paste" actions -- while the core of data curatorial work may appear to a computer like a sequence of insertions, deletions and copies
- additionally: need to support visual rearrangement and browsing of tables and tuples; this might have SQL at its core but needs to be more user friendly. Bulk of HCI/UX studies have been about query construction - less has been written about user-friendliness of database GUIs, particularly from a curatorial perspective (e.g. someone doing quality assessment, editing, as opposed to just data entry, or just information retrieval
- need support for review of students'/volunteers' data entry  -- right now there's no great way to handle this.

\textbf{future work:}
- logical modeling as a design problem for CSCW
- in what ways do people come to know an information system? we need to study not just long lived databases but also people who work with databases for a longer period of time - people who gain almost tactile knowledge of information systems based on consistent use (as opposed to studies of how naive users use a database once).
- We'd like to study comprable organizations with relational databases.